
@inproceedings{gong_reinforcement_2023,
	title = {Reinforcement {Learning} {Enabled} {Real}-{Time} {Energy} {Dispatch} of {Energy} {Systems} with {Flexible} {Operational} {Resources}},
	doi = {10.1109/AIKIIE60097.2023.10390385},
	abstract = {This paper developed a reinforcement learning- enabled real-time energy dispatch solution for energy systems with flexible operational resources. The detailed process of the proposed solution is formulated and implemented. The deep reinforcement learning method is introduced to solve the action strategy of the energy storage system and the controllable generator. Specifially, this work developed a reinforcement learning-enabled real-time energy dispatch solution for energy systems considering the presence of flexible operational resources. For the real-time scheduling phase, the timing decision problem of real-time scheduling is designed to maintain the stability of the trading curve. The deep reinforcement learning algorithm is designed and adopted to identify the action strategy of the energy storage system and the controllable generator. The proposed solution is evaluated through a case study via simulations and the numerical results confirmed the effectiveness of the proposed real-time energy dispatch solution for multi-energy systems.},
	booktitle = {2023 {International} {Conference} on {Ambient} {Intelligence}, {Knowledge} {Informatics} and {Industrial} {Electronics} ({AIKIIE})},
	author = {Gong, Liwu and Huang, Yuehua and Zhang, Wei and Gu, Yixing and Chen, Chao},
	month = nov,
	year = {2023},
	keywords = {Optimization, Real-time systems, Reinforcement learning, Reinforcement Learning, Deep learning, Generators, Control systems, Day-Ahead Energy Planning, Job shop scheduling, Multi-Energy Systems, Stability analysis},
	pages = {1--6},
	file = {Gong et al. - 2023 - Reinforcement Learning Enabled Real-Time Energy Di.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\DEL4RFMN\\Gong et al. - 2023 - Reinforcement Learning Enabled Real-Time Energy Di.pdf:application/pdf},
}

@inproceedings{selim_optimal_2022,
	title = {Optimal {Scheduled} {Control} {Operation} of {Battery} {Energy} {Storage} {System} using {Model}-{Free} {Reinforcement} {Learning}},
	doi = {10.1109/iSPEC54162.2022.10033035},
	abstract = {Driven by the tremendous increase in rooftop solar panels and battery installations in Australian states, several studies have been conducted to efficiently manage battery operations with the imported grid power through battery energy storage systems (BESS). Therefore, it is crucial for the BESS to carefully decide the power set-points of the installed batteries to maintain user comfort while operating household appliances. Additionally, BESS should be capable of reducing the electricity bills by optimally managing the battery operation to sustain times of higher tariff prices for the imported grid power. This paper formulates the scheduled operation of the BESS as a Markov decision Process (MDP) that enables the BESS to Figure out numerous scenarios and decides the optimal power set-points for both batteries and grid power. A model-free reinforcement learning approach is proposed to manage the batteries’ power-sharing and grid operation set-points to solve this MDP problem. This approach utilizes the advantages of the Deep Deterministic Policy Gradient (DDPG) algorithm to decide the shared power set-points every 5 minutes interval for the day ahead operation of the BESS. Finally, the proposed model is trained and validated using historical data of the Australian National Electricity market to offer an optimal scheduled control pattern for the daily BESS operations.},
	booktitle = {2022 {IEEE} {Sustainable} {Power} and {Energy} {Conference} ({iSPEC})},
	author = {Selim, Alaa},
	month = dec,
	year = {2022},
	keywords = {Pricing, Reinforcement learning, Optimal scheduling, Tariffs, Training, Battery energy storage system, Control optimization, Model-free, Optimal control, Power sharing, State of charge, Supercomputers},
	pages = {1--5},
	file = {Selim - 2022 - Optimal Scheduled Control Operation of Battery Ene.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\X6TBK9SI\\Selim - 2022 - Optimal Scheduled Control Operation of Battery Ene.pdf:application/pdf},
}

@inproceedings{basso_battery_2023,
	title = {Battery {Energy} {Storage} {Control} {Using} {Reinforcement} {Learning}},
	doi = {10.1109/IFEEC58486.2023.10458579},
	abstract = {With the increasing adoption of solar PV installations in Australian households, the availability of cheap renewable power during the day has surged. However, the challenge lies in rising electricity prices during morning and evening peak-consumption times. This project assessed the feasibility and profitability of using a Reinforcement Learning (RL) controller in a Battery Energy Storage System (BESS) to make cost-effective decisions by purchasing power when it's inexpensive and selling when it's costly. MATLAB/Simulink is used to create a BESS simulation model integrated into the electricity market, with the RL agent trained using normalized observation data and a reward function. Benchmarking demonstrated the RL controller's consistent outperformance of the timer-based controller in various market scenarios, emphasizing its adaptability and profitability advantages, particularly in volatile markets.},
	booktitle = {2023 {IEEE} {International} {Future} {Energy} {Electronics} {Conference} ({IFEEC})},
	author = {Basso, Elliott and Du, Yang},
	month = nov,
	year = {2023},
	keywords = {Reinforcement learning, Mathematical models, Electricity supply industry, Reinforcement Learning, Renewable energy sources, Profitability, Battery Energy Storage System, Energy consumption, Fluctuations, Matlab/Simulink},
	pages = {1--5},
	file = {Basso and Du - 2023 - Battery Energy Storage Control Using Reinforcement.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\DFIK6QT2\\Basso and Du - 2023 - Battery Energy Storage Control Using Reinforcement.pdf:application/pdf},
}

@inproceedings{florez_ai-optimized_2023,
	address = {Cham},
	title = {{AI}-{Optimized} {Energy} {Management} for {More} {Efficient} and {Sustainable} {Microgrids}},
	isbn = {978-3-031-38318-2},
	doi = {10.1007/978-3-031-38318-2_43},
	abstract = {The implementation of renewable energy sources reduces dependence on fossil fuels and greenhouse gas emissions, but it comes at a significant cost to the grid due to the need for coupling multiple energy sources. This necessitates the optimization of energy production using artificial intelligence techniques that can reflect the characteristics of distributed energy management from the storage capacity of the microgrid while considering the consumer load. As such, this study develops an energy management algorithm based on the Deep-Q-Network (DQN) with prioritized experience replay (PER) utility function to model the operator’s risk-aversion or risk-seeking behavior, generating optimal exchange strategies in a dynamic environment. The reinforcement learning-based agent provides an understanding of energy storage capacity constraints in aggregate load/discharge energy decision making in the microgrid, using a discrete action space that depends on a reward related to the value of the optimal online objective function of the microgrid. By analyzing microgrid management under different system state definitions, the results show that substantially improved performance can be achieved compared to a traditional model that assumes deterministic information. The findings provide solid experimental validation of the potential of deep reward-based learning techniques to enable more efficient, flexible, and cost-effective energy management, particularly with the integration of more intermittent renewable energy sources.},
	language = {en},
	booktitle = {Distributed {Computing} and {Artificial} {Intelligence}, {Special} {Sessions} {I}, 20th {International} {Conference}},
	publisher = {Springer Nature Switzerland},
	author = {Flórez, Sebastián López and Herniández, Guillermo and Gonziález-Briones, Alfonso and de la Prieta, Fernando},
	editor = {Mehmood, Rashid and Alves, Victor and Praça, Isabel and Wikarek, Jarosław and Parra-Domínguez, Javier and Loukanova, Roussanka and de Miguel, Ignacio and Pinto, Tiago and Nunes, Ricardo and Ricca, Michela},
	year = {2023},
	keywords = {Reinforcement learning, contingency reserve, Energy efficiency optimization, Smart grid technologies},
	pages = {438--447},
	file = {Full Text PDF:C\:\\Users\\Jacob\\Zotero\\storage\\5ZRJMEMB\\Flórez et al. - 2023 - AI-Optimized Energy Management for More Efficient .pdf:application/pdf},
}

@article{el_bourakadi_robust_2023,
	title = {A robust energy management approach in two-steps ahead using deep learning {BiLSTM} prediction model and type-2 fuzzy decision-making controller},
	volume = {22},
	issn = {1573-2908},
	url = {https://doi.org/10.1007/s10700-022-09406-y},
	doi = {10.1007/s10700-022-09406-y},
	abstract = {The price prediction is valuable in energy management system (EMS) because it allows making informed decisions and solving the problem of the uncertainty related to the future ignorance based only on the past knowledge. To this goal, we present in this paper a two-steps EMS in order to control the different operations of a micro-grid (MG). In the first step, we exploit the advantages of the Bidirectional Long-Short Term Memory (BiLSTM) deep learning model to predict the next daily electricity price based on time series. In the second step, we use a type-2 fuzzy logic controller to decide which energy source will exploit the excess energy produced or meet the MG need. Real data is used in this paper to test the effectiveness of the proposed EMS whose superiority is proved through the test period. The BiLSTM forecasting model better performs compared to other related algorithms designed to the electricity price prediction. In addition, the proposed decision-making process can reduce the total MG cost and protect the batteries against the deep discharge and maximum charge in order to prolong their lifespan. We expect that this work can contribute to meet the real-world needs in the management of the electrical system.},
	language = {en},
	number = {4},
	urldate = {2024-07-19},
	journal = {Fuzzy Optimization and Decision Making},
	author = {El Bourakadi, Dounia and Ramadan, Hiba and Yahyaouy, Ali and Boumhidi, Jaouad},
	month = dec,
	year = {2023},
	keywords = {Energy management system, Bidirectional long-short term memory, Decision-making, Electricity price prediction, Type-2 fuzzy logic control},
	pages = {645--667},
	file = {Full Text PDF:C\:\\Users\\Jacob\\Zotero\\storage\\EBVRFY8K\\El Bourakadi et al. - 2023 - A robust energy management approach in two-steps a.pdf:application/pdf},
}

@inproceedings{bousnina_deep_2022,
	address = {Cham},
	title = {Deep {Reinforcement} {Learning} for {Optimal} {Energy} {Management} of {Multi}-energy {Smart} {Grids}},
	isbn = {978-3-030-95470-3},
	doi = {10.1007/978-3-030-95470-3_2},
	abstract = {This paper proposes a Deep Reinforcement Learning approach for optimally managing multi-energy systems in smart grids. The optimal control problem of the production and storage units within the smart grid is formulated as a Partially Observable Markov Decision Process (POMDP), and is solved using an actor-critic Deep Reinforcement Learning algorithm. The framework is tested on a novel multi-energy residential microgrid model that encompasses electrical, heating and cooling storage as well as thermal production systems and renewable energy generation. One of the main challenges faced when dealing with real-time optimal control of such multi-energy systems is the need to take multiple continuous actions simultaneously. The proposed Deep Deterministic Policy Gradient (DDPG) agent has shown to handle well the continuous state and action spaces and learned to simultaneously take multiple actions on the production and storage systems that allow to jointly optimize the electrical, heating and cooling usages within the smart grid. This allows the approach to be applied for the real-time optimal energy management of larger scale multi-energy Smart Grids like eco-distrits and smart cities where multiple continuous actions need to be taken simultaneously.},
	language = {en},
	booktitle = {Machine {Learning}, {Optimization}, and {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Bousnina, Dhekra and Guerassimoff, Gilles},
	editor = {Nicosia, Giuseppe and Ojha, Varun and La Malfa, Emanuele and La Malfa, Gabriele and Jansen, Giorgio and Pardalos, Panos M. and Giuffrida, Giovanni and Umeton, Renato},
	year = {2022},
	keywords = {Smart grids, Energy management, Deep Reinforcement Learning, Actor-critic, Multi-energy},
	pages = {15--30},
	file = {Full Text PDF:C\:\\Users\\Jacob\\Zotero\\storage\\DJ464Q2N\\Bousnina and Guerassimoff - 2022 - Deep Reinforcement Learning for Optimal Energy Man.pdf:application/pdf},
}

@article{si_deep_2021,
	title = {Deep reinforcement learning based home energy management system with devices operational dependencies},
	volume = {12},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-020-01266-5},
	doi = {10.1007/s13042-020-01266-5},
	abstract = {Advanced metering infrastructure and bilateral communication technologies facilitate the development of the home energy management system in the smart home. In this paper, we propose an energy management strategy for controllable loads based on reinforcement learning (RL). First, based on the mathematical model, the Markov decision process of different types of home energy resources (HERs) is formulated. Then, two RL algorithms, i.e. deep Q-learning and deep deterministic policy gradient are utilized. Based on the living habits of the residents, the dependency modes for HERs are proposed and are integrated into the reinforcement learning algorithms. Through the case studies, it is verified that the proposed method can schedule HERs properly to satisfy the established dependency modes. The difference between the achieved result and the optimal solution is relatively small.},
	language = {en},
	number = {6},
	urldate = {2024-07-19},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Si, Caomingzhe and Tao, Yuechuan and Qiu, Jing and Lai, Shuying and Zhao, Junhua},
	month = jun,
	year = {2021},
	keywords = {Reinforcement learning, Smart home, Dependency modes, Home energy management system},
	pages = {1687--1703},
	file = {Full Text PDF:C\:\\Users\\Jacob\\Zotero\\storage\\SIFIQSGW\\Si et al. - 2021 - Deep reinforcement learning based home energy mana.pdf:application/pdf},
}

@article{berlink_intelligent_2015,
	title = {Intelligent {Decision}-{Making} for {Smart} {Home} {Energy} {Management}},
	volume = {80},
	issn = {1573-0409},
	url = {https://doi.org/10.1007/s10846-014-0169-8},
	doi = {10.1007/s10846-014-0169-8},
	abstract = {One of the goals of Smart Grids is to encourage distributed generation of energy in houses, hence allowing the user to profit by injecting energy into the power grid. The implementation of a differentiated tariff of energy per time of use, coupled with energy storage in batteries, enables profit maximization by the user, who can choose to sell or store the energy generated whenever it is convenient. This paper proposes a solution to the sequential decision-making problem of energy sale by applying reinforcement learning. Results show a significant increase in the total long-term profit by using the policy obtained with the proposed approach, when compared with a price-unaware selling policy.},
	language = {en},
	number = {1},
	urldate = {2024-07-19},
	journal = {Journal of Intelligent \& Robotic Systems},
	author = {Berlink, Heider and Kagan, Nelson and Reali Costa, Anna Helena},
	month = dec,
	year = {2015},
	keywords = {Reinforcement learning, Energy management system, SmartGrid, SmartHome},
	pages = {331--354},
	file = {Full Text PDF:C\:\\Users\\Jacob\\Zotero\\storage\\W2RBC8NG\\Berlink et al. - 2015 - Intelligent Decision-Making for Smart Home Energy .pdf:application/pdf},
}

@article{yi_integrated_2022,
	title = {An integrated energy management system using double deep {Q}-learning and energy storage equipment to reduce energy cost in manufacturing under real-time pricing condition: {A} case study of scale-model factory},
	volume = {38},
	issn = {1755-5817},
	doi = {10.1016/j.cirpj.2022.07.009},
	abstract = {Reducing energy costs is an emerging aspect in the research on the economic and environmental dimen-sions of manufacturing systems. The share of electricity cost accounts for approximately 60 \% of the total energy cost of a manufacturing system, whereas the share of oil, coal, and gas accounts for the remaining 40 \%. The electricity cost is dependent on the electricity price and usage. In terms of the electricity price, one of the pricing strategies widely used in the USA and Europe is called real-time pricing (RTP), which is characterised by hourly price changes. Compared to other pricing strategies, RTP yields the highest reward and the highest risk. In the RTP strategy, the electricity price is influenced by the supply and demand of the energy market. Hence, the energy cost of manufacturing cannot be determined by the manufacturing companies, implying a high level of risk. However, if manufacturing companies seize the opportunity to perform more manufacturing tasks when the energy price is low, the cost-savings will be significant, im-plying a high level of reward. In this study, we propose an integrated energy management system (IEMS) to reduce the energy cost of manufacturing systems. The IEMS consists of an energy storage equipment and an intelligent switch mechanism. When the electricity price is high, the manufacturing system is powered by the energy storage equipment. When the electricity price is low, the manufacturing system is powered by the public electricity grid, and the energy storage equipment is charged. The decision-making of these operations is performed by the intelligent switch mechanism based on double deep Q-learning. To validate this framework, a case study is conducted, in which an IEMS is developed to reduce the electricity cost of a scale-model factory. Based on an online test of the IEMS in different manufacturing cycles, it is concluded that the proposed IEMS approach achieves a cost reduction of approximately 57.21 \%.(c) 2022 The Authors. CC\_BY\_NC\_ND\_4.0},
	journal = {CIRP JOURNAL OF MANUFACTURING SCIENCE AND TECHNOLOGY},
	author = {Yi, Li and Langlotz, Pascal and Hussong, Marco and Glatt, Moritz and Sousa, Fabio J. P. and Aurich, Jan C.},
	month = aug,
	year = {2022},
	keywords = {Reinforcement learning, Energy management system, Double deep Q-learning, Energy cost, Manufacturing system, Real-time pricing},
	pages = {844--860},
	file = {Yi et al. - 2022 - An integrated energy management system using doubl.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\6F4UGQV9\\Yi et al. - 2022 - An integrated energy management system using doubl.pdf:application/pdf},
}

@article{samadi_decentralized_2020,
	title = {Decentralized multi-agent based energy management of microgrid using reinforcement learning},
	volume = {122},
	issn = {0142-0615},
	doi = {10.1016/j.ijepes.2020.106211},
	abstract = {This paper proposes a multi-agent based decentralized energy management approach in a grid-connected microgrid (MG). The MG comprises of wind and photovoltaic resources, diesel generator, electrical energy storage, and combined heat and power generations to serve electrical and thermal loads at the lower-level of energy management system (EMS). All distributed energy resources (DERs) and customers are modelled as self-interested agents who adopt reinforcement learning to optimize their behaviours and operation costs. Based on this algorithm, agents have the capability to interact with each other in a distributed manner and find the best strategy in competitive environment. At the upper-level of EMS, there is an energy management agent that gathers the information of agents of lower-level and clears the MG electrical and thermal energy market in line with predetermined goals. Utilizing energy availability from different DERs and variety of customers' consumption patterns, considering uncertainty of renewable generation and load consumption and taking into account technical constraint of DERs are the strengths of the presented framework. Performance of the proposed algorithm is investigated under different conditions of agents learning and using epsilon-greedy, soft-max and upper confidence bound methods. The simulation results verify efficacy of the proposed approach.},
	journal = {INTERNATIONAL JOURNAL OF ELECTRICAL POWER \& ENERGY SYSTEMS},
	author = {Samadi, Esmat and Badri, Ali and Ebrahimpour, Reza},
	month = nov,
	year = {2020},
	keywords = {Reinforcement learning, Distributed energy resources, Microgrid energy management system, Multi-agent systems},
	file = {Samadi et al. - 2020 - Decentralized multi-agent based energy management .pdf:C\:\\Users\\Jacob\\Zotero\\storage\\H328U336\\Samadi et al. - 2020 - Decentralized multi-agent based energy management .pdf:application/pdf},
}

@article{hussain_deep_2022,
	title = {Deep reinforcement learning-based operation of fast charging stations coupled with energy storage system},
	volume = {210},
	issn = {0378-7796},
	doi = {10.1016/j.epsr.2022.108087},
	abstract = {Fast charging stations (FCSs) can reduce the charging time of electric vehicles (EVs) and thus can help in the widespread adoption of EVs. However, FCSs may result in the power system overload. Therefore, the deployment of the battery energy storage system (BESS) in FCSs is considered as a potential solution to avoid system overload. However, the optimal operation of FCSs equipped with BESS is challenging due to the involvement of several uncertainties, such as EV arrival/departure times and electricity prices. Therefore, in this study, a deep reinforcement learning-based method is proposed to operate FCSs with BESS under these uncertainties. The stateof-the-art soft actor-critic method (SAC) is adopted and the model is trained with one-year data to cover seasonality and different types of days (working days and holidays). The performance of SAC is compared with two other deep reinforcement learning methods, i.e., deep deterministic policy gradient and twin delayed deep deterministic policy gradient. A comprehensive reward function is devised to train the model offline, which can then be used for the real-time operation of FCS with BESS under different uncertainties. The trained model has successfully reduced the peak load of the FCS during both weekdays and holidays by optimizing the operation of the BESS. In addition, the robustness of the proposed model against different EV arrival scenarios and extreme market price scenarios is also evaluated. Simulation results have shown that the proposed model can reduce the peak load of the FCS under diverse conditions in the desired fashion.},
	journal = {ELECTRIC POWER SYSTEMS RESEARCH},
	author = {Hussain, Akhtar and Bui, Van-Hai and Kim, Hak-Man},
	month = sep,
	year = {2022},
	keywords = {Reinforcement learning, Deep reinforcement learning, Smart grid, Electric vehicles, Charging station, Energy storage system},
	file = {Hussain et al. - 2022 - Deep reinforcement learning-based operation of fas.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\BDBFJCQC\\Hussain et al. - 2022 - Deep reinforcement learning-based operation of fas.pdf:application/pdf},
}

@article{ochoa_multi-agent_2022,
	title = {Multi-agent deep reinforcement learning for efficient multi-timescale bidding of a hybrid power plant in day-ahead and real-time markets},
	volume = {317},
	issn = {0306-2619},
	doi = {10.1016/j.apenergy.2022.119067},
	abstract = {Effective bidding on multiple electricity products under uncertainty would allow a more profitable market participation for hybrid power plants with variable energy resources and storage systems, therefore aiding the decarbonization process. This study deals with the effective bidding of a photovoltaic plant with an energy storage system (PV-ESS) participating in multi-timescale electricity markets by providing energy and ancillary services (AS) products. The energy management system (EMS) aims to maximize the plant's profits by efficiently bidding in the day-ahead and real-time markets while considering the awarded products' adequate delivery. EMS's bidding decisions are usually obtained from traditional mathematical optimization frameworks. However, since the addressed problem is a multi-stage stochastic program, it is often intractable and suffers the curse of dimensionality. This paper presents a novel multi-agent deep reinforcement learning (MADRL) framework for efficient multi-timescale bidding. Two agents based on multi-view artificial neural networks with recurrent layers (MVANNs) are adjusted to map environment observations to actions. Such mappings use as inputs available information related to electricity market products, bidding decisions, solar generation, stored energy, and time representations to bid in both electricity markets. Sustained by a price-taker assumption, the physically and financially constrained EMS's environment is simulated by employing historical data. A shared cumulative reward function with a finite time horizon is used to adjust both MVANNs' weights simultaneously during the learning phase. We compare the proposed MADRL framework against scenario-based two-stage robust and stochastic optimization methods. Results are provided for one-year-round market participation of the hybrid plant at a 1-minute resolution. The proposed method achieved statistically significant higher profits, less variable incomes from both electricity markets, and better provision of awarded products by achieving smaller and less variable energy imbalances through time.},
	journal = {APPLIED ENERGY},
	author = {Ochoa, Tomas and Gil, Esteban and Angulo, Alejandro and Valle, Carlos},
	month = jul,
	year = {2022},
	keywords = {Energy storage, Energy management system, Electricity market bidding, Multi-agent deep reinforcement learning, Multi-timescale electricity markets, Multi-view artificial neural networks, Solar generation},
	file = {Ochoa et al. - 2022 - Multi-agent deep reinforcement learning for effici.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\YSA8F8LA\\Ochoa et al. - 2022 - Multi-agent deep reinforcement learning for effici.pdf:application/pdf},
}

@article{taherian_optimal_2021,
	title = {Optimal dynamic pricing for an electricity retailer in the price-responsive environment of smart grid},
	volume = {130},
	issn = {0142-0615},
	doi = {10.1016/j.ijepes.2021.107004},
	abstract = {The main purpose of this study is to support a retail electric provider (REP) to make the best day-ahead dynamic pricing decisions in a realistic scenario. These decisions are made with the aim of maximizing the profit achieved by the REP under the assumption that mixed types of customers with different behaviors in the electricity market are considered. While some of the customers have installed smart meters with an embedded home energy management system (HEMS) in their home, others do not participate in the demand response (DR) programs. For this purpose, a bi-level hybrid demand modeling framework is proposed. It firstly uses an optimal energy management algorithm with bill minimization in order to model the behavior of customers with smart meters. Then, using a customers? behavior learning machine (CBLM), the behavior of other groups without smart meters is modeled. Therefore, the proposed hybrid model cannot only schedule usage of home appliances to the interests of customers with smart meters but can also be used to understand electricity usage behavior of customers without smart meters. The proposed model includes a stacked auto-encoder (SAE), one of the deep learning (DL) methods suitable for real-valued inputs, and adaptive neuro-fuzzy inference system (ANFIS). Based on the established hybrid demand model for all customers, a profit maximization algorithm is developed in order to achieve optimal prices for the REP under relevant market constraints. The results of the case studies confirm the applicability and effectiveness of the proposed model.},
	journal = {INTERNATIONAL JOURNAL OF ELECTRICAL POWER \& ENERGY SYSTEMS},
	author = {Taherian, Hossein and Aghaebrahimi, Mohammad Reza and Baringo, Luis and Goldani, Saeid Reza},
	month = sep,
	year = {2021},
	keywords = {Deep learning, Smart grid, Bidding strategy, Customer behavior learning, Day-ahead dynamic pricing, Retail electric provider},
	file = {Taherian et al. - 2021 - Optimal dynamic pricing for an electricity retaile.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\GZV7UYVL\\Taherian et al. - 2021 - Optimal dynamic pricing for an electricity retaile.pdf:application/pdf},
}

@article{kelly_optimal_2020,
	title = {Optimal investment timing and sizing for battery energy storage systems},
	volume = {28},
	doi = {10.1016/j.est.2020.101272},
	abstract = {Due to electricity market deregulation over the past two decades, the responsibility for new generation is with private investors who seek profit maximisation. Battery Energy Storage Systems (BESS), which are one solution to combat the intermittent nature of renewable energy sources, also require private investment for widespread deployment. This paper develops a methodology for applying Real Options Analysis to a BESS project from the perspective of private investors to determine the optimal investment time and BESS capacity size (MWh). Two models with different timescales are utilized: the operational model which is hourly, and the planning model which is yearly. The operational model is solved using a reinforcement learning algorithm called Deterministic Policy Gradient, while the planning model is solved using a MATLAB inbuilt nonlinear global optimiser called patternsearch. The methodology is demonstrated for a 100 MW BESS connected to the Irish grid and trading exclusively in the day-ahead market. Three different BESS CAPEX future realisations are analysed along with three different BESS manufacturers' degradation warranties for C-Rates under 0.37C. The results show that BESS CAPEX has minimal influence on investment timing but has a significant effect on BESS size. Furthermore, extrapolating degradation warranty for C-Rates greater than 0.37C does not influence optimal investment timing or sizing, while a change in BESS energy retention limit at year 10 can have a significant influence on the viability of a BESS project.},
	journal = {JOURNAL OF ENERGY STORAGE},
	author = {Kelly, Joseph J. and Leahy, Paul G.},
	month = apr,
	year = {2020},
	keywords = {Reinforcement learning, Battery degradation, Battery energy storage systems, Deterministic Policy Gradient Algorithm, Net Present Value, Real Option Analysis, Stochastic optimization},
	file = {Kelly and Leahy - 2020 - Optimal investment timing and sizing for battery e.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\XVTDH4UD\\Kelly and Leahy - 2020 - Optimal investment timing and sizing for battery e.pdf:application/pdf},
}

@article{xie_multi-agent_2023,
	title = {Multi-{Agent} attention-based deep reinforcement learning for demand response in grid-responsive buildings},
	volume = {342},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261923005263},
	doi = {https://doi.org/10.1016/j.apenergy.2023.121162},
	abstract = {Integrating renewable energy resources and deploying energy management devices offer great opportunities to develop autonomous energy management systems in grid-responsive buildings. Demand response can promote enhancing demand flexibility and energy efficiency while reducing consumer costs. In this work, we propose a novel multi-agent deep reinforcement learning (MADRL) based approach with an agent assigned to individual buildings to facilitate demand response programs with diverse loads, including space heating/cooling and electrical equipment. Achieving real-time autonomous demand response in networks of buildings is challenging due to uncertain system parameters, the dynamic market price, and complex coupled operational constraints. To develop a scalable approach for automated demand response in networks of interconnected buildings, coordination between buildings is necessary to ensure demand flexibility and the grid's stability. We propose a MADRL technique that utilizes an actor-critic algorithm incorporating shared attention mechanism to enable effective and scalable real-time coordinated demand response in grid-responsive buildings. The presented case studies demonstrate the ability of the proposed approach to obtain decentralized cooperative policies for electricity costs minimization and efficient load shaping without knowledge of building energy systems. The viability of the proposed control approach is also demonstrated by a reduction of over 6\% net load demand compared to standard reinforcement learning approaches, deep deterministic policy gradient, and soft actor-critic algorithm, as well as a tailored MADRL approach for demand response.},
	journal = {Applied Energy},
	author = {Xie, Jiahan and Ajagekar, Akshay and You, Fengqi},
	year = {2023},
	keywords = {Deep reinforcement learning, Demand response, Buildings, Multi-agent},
	pages = {121162},
	file = {Xie et al. - 2023 - Multi-Agent attention-based deep reinforcement lea.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\BL5A3TXR\\Xie et al. - 2023 - Multi-Agent attention-based deep reinforcement lea.pdf:application/pdf},
}

@article{dreher_ai_2022,
	title = {{AI} agents envisioning the future: {Forecast}-based operation of renewable energy storage systems using hydrogen with {Deep} {Reinforcement} {Learning}},
	volume = {258},
	issn = {0196-8904},
	url = {https://www.sciencedirect.com/science/article/pii/S0196890422001972},
	doi = {https://doi.org/10.1016/j.enconman.2022.115401},
	abstract = {Hydrogen-based energy storage has the potential to compensate for the volatility of renewable power generation in energy systems with a high renewable penetration. The operation of these storage facilities can be optimized using automated energy management systems. This work presents a Reinforcement Learning-based energy management approach in the context of CO2-neutral hydrogen production and storage for an industrial combined heat and power application. The economic performance of the presented approach is compared to a rule-based energy management strategy as a lower benchmark and a Dynamic Programming-based unit commitment as an upper benchmark. The comparative analysis highlights both the potential benefits and drawbacks of the implemented Reinforcement Learning approach. The simulation results indicate a promising potential of Reinforcement Learning-based algorithms for hydrogen production planning, outperforming the lower benchmark. Furthermore, a novel approach in the scientific literature demonstrates that including energy and price forecasts in the Reinforcement Learning observation space significantly improves optimization results and allows the algorithm to take variable prices into account. An unresolved challenge, however, is balancing multiple conflicting objectives in a setting with few degrees of freedom. As a result, no parameterization of the reward function could be found that fully satisfied all predefined targets, highlighting one of the major challenges for Reinforcement Learning -based energy management algorithms to overcome.},
	journal = {Energy Conversion and Management},
	author = {Dreher, Alexander and Bexten, Thomas and Sieker, Tobias and Lehna, Malte and Schütt, Jonathan and Scholz, Christoph and Wirsum, Manfred},
	year = {2022},
	keywords = {Deep reinforcement learning, Energy management, Hydrogen, Dynamic programming, Renewable energy storage},
	pages = {115401},
	file = {Dreher et al. - 2022 - AI agents envisioning the future Forecast-based o.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\TZ6IXY95\\Dreher et al. - 2022 - AI agents envisioning the future Forecast-based o.pdf:application/pdf},
}

@article{miao_co-optimizing_2021,
	title = {Co-{Optimizing} {Battery} {Storage} for {Energy} {Arbitrage} and {Frequency} {Regulation} in {Real}-{Time} {Markets} {Using} {Deep} {Reinforcement} {Learning}},
	volume = {14},
	doi = {10.3390/en14248365},
	abstract = {Battery energy storage systems (BESSs) play a critical role in eliminating uncertainties associated with renewable energy generation, to maintain stability and improve flexibility of power networks. In this paper, a BESS is used to provide energy arbitrage (EA) and frequency regulation (FR) services simultaneously to maximize its total revenue within the physical constraints. The EA and FR actions are taken at different timescales. The multitimescale problem is formulated as two nested Markov decision process (MDP) submodels. The problem is a complex decision-making problem with enormous high-dimensional data and uncertainty (e.g., the price of the electricity). Therefore, a novel co-optimization scheme is proposed to handle the multitimescale problem, and also coordinate EA and FR services. A triplet deep deterministic policy gradient with exploration noise decay (TDD-ND) approach is used to obtain the optimal policy at each timescale. Simulations are conducted with real-time electricity prices and regulation signals data from the American PJM regulation market. The simulation results show that the proposed approach performs better than other studied policies in literature.},
	number = {24},
	journal = {ENERGIES},
	author = {Miao, Yushen and Chen, Tianyi and Bu, Shengrong and Liang, Hao and Han, Zhu},
	month = dec,
	year = {2021},
	file = {1-s2.0-S0925231220304367-main.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\X8D5Z3WX\\1-s2.0-S0925231220304367-main.pdf:application/pdf;Full Text:C\:\\Users\\Jacob\\Zotero\\storage\\MBBP8FR9\\Miao et al. - 2021 - Co-Optimizing Battery Storage for Energy Arbitrage.pdf:application/pdf},
}

@inproceedings{anwar_proximal_2022,
	title = {Proximal {Policy} {Optimization} {Based} {Reinforcement} {Learning} for {Joint} {Bidding} in {Energy} and {Frequency} {Regulation} {Markets}},
	doi = {10.1109/PESGM48719.2022.9917082},
	abstract = {Driven by the global decarbonization effort, the rapid integration of renewable energy into the conventional electricity grid presents new challenges and opportunities for the battery energy storage system (BESS) participating in the energy market. Energy arbitrage can be a significant source of revenue for the BESS due to the increasing price volatility in the spot market caused by the mismatch between renewable generation and electricity demand. In addition, the Frequency Control Ancillary Services (FCAS) markets established to stabilize the grid can offer higher returns for the BESS due to their capability to respond within milliseconds. Therefore, it is crucial for the BESS to carefully decide how much capacity to assign to each market to maximize the total profit under uncertain market conditions. This paper formulates the bidding problem of the BESS as a Markov Decision Process, which enables the BESS to participate in both the spot market and the FCAS market to maximize profit. Then, Proximal Policy Optimization, a model-free deep reinforcement learning algorithm, is employed to learn the optimal bidding strategy from the dynamic environment of the energy market under a continuous bidding scale. The proposed model is trained and validated using real-world historical data of the Australian National Electricity Market. The results demonstrate that our developed joint bidding strategy in both markets is significantly profitable compared to individual markets.},
	booktitle = {2022 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting} ({PESGM})},
	author = {Anwar, Muhammad and Wang, Changlong and de Nijs, Frits and Wang, Hao},
	month = jul,
	year = {2022},
	note = {ISSN: 1944-9933},
	keywords = {Heuristic algorithms, Reinforcement learning, Electricity supply industry, Renewable energy sources, Profitability, Regulation, Markov processes},
	pages = {1--5},
	file = {Submitted Version:C\:\\Users\\Jacob\\Zotero\\storage\\5VHT5SPC\\Anwar et al. - 2022 - Proximal Policy Optimization Based Reinforcement L.pdf:application/pdf},
}

@article{tightiz_resilience_2021,
	title = {Resilience {Microgrid} as {Power} {System} {Integrity} {Protection} {Scheme} {Element} {With} {Reinforcement} {Learning} {Based} {Management}},
	volume = {9},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2021.3087491},
	abstract = {The microgrid is a solution for integrating renewable energy resources into the power system. However, overcoming the randomness of these nature-based resources requires a robust control system. Moreover, electricity market participation and ancillary service provision for the utility grid are other aspects, although intensify microgrid penetration makes its environment interactions more complex. Reinforcement learning is a technique vastly applied to such an intricate environment. Hence, in this paper, we deployed deep deterministic policy gradient and soft-actor critic methods to solve the high-dimensional, continuous, and stochastic problem of the microgrid's energy management system and compared the performance of two methods. Additionally, we developed the microgrid interactions with the utility grid as a participant of system integrity protection schema responding promptly to the utility grid protection requirements based on its reliable available resources. Moreover, we applied actual data of Gasa Island microgrid in Korea to prove the efficiency of proposed method.},
	journal = {IEEE Access},
	author = {Tightiz, Lilia and Yang, Hyosik},
	year = {2021},
	keywords = {Batteries, Uncertainty, Electricity supply industry, Energy management, Microgrids, Energy management system, Energy management systems, soft actor-critic, deep deterministic policy gradient, Power generation, system integrity protection schema},
	pages = {83963--83975},
	file = {Full Text:C\:\\Users\\Jacob\\Zotero\\storage\\PRWWL3QS\\Tightiz and Yang - 2021 - Resilience Microgrid as Power System Integrity Pro.pdf:application/pdf},
}

@inproceedings{dreher_ai_2022-1,
	title = {{AI} agents assessing flexibility: the value of demand side management in times of high energy prices},
	doi = {10.1109/EEM54602.2022.9920982},
	abstract = {High energy and electricity prices, coupled with high price volatility, increase the value of demand response and demand side management. Energy management systems that are predictive and exchange-price oriented can help to leverage flexibility while lowering energy costs. In this paper, the value of flexibility is assessed in two scenarios of low and high energy prices. To that end, a self-learning home energy management system is introduced that takes into account the new volatility of high stock market electricity prices. The proposed approach is compared to a baseline system, which is a typical household self-consumption optimization. The findings indicate a significant economic potential of an exchange-price oriented usage of residential flexibility options in contrast to self-consumption optimization. Furthermore, the value of flexibility for the exemplary residential system increased more than fivefold between 2021 and 2022, while prices increased about fourfold.},
	booktitle = {2022 18th {International} {Conference} on the {European} {Energy} {Market} ({EEM})},
	author = {Dreher, Alexander and Martmann, Lisa Marie and Lehna, Malte and Roelofs, Cyriana and Bergsträßer, Jonathan and Scholz, Christoph and Slaby, Wolfgang and Wetzel, Heike},
	month = sep,
	year = {2022},
	note = {ISSN: 2165-4093},
	keywords = {Electric potential, Costs, Europe, Reinforcement Learning, Demand response, Energy management systems, Energy Management, Demand side management, Demand Side Management, Power System Economics, Stock markets},
	pages = {1--9},
	file = {Dreher et al. - 2022 - AI agents assessing flexibility the value of dema.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\S5QH9TWI\\Dreher et al. - 2022 - AI agents assessing flexibility the value of dema.pdf:application/pdf},
}

@inproceedings{li_optimal_2023,
	title = {Optimal {Energy} {Storage} {Scheduling} for {Wind} {Curtailment} {Reduction} and {Energy} {Arbitrage}: {A} {Deep} {Reinforcement} {Learning} {Approach}},
	doi = {10.1109/PESGM52003.2023.10253181},
	abstract = {Wind energy has been rapidly gaining popularity as a means for combating climate change. However, the variable nature of wind generation can undermine system reliability and lead to wind curtailment, causing substantial economic losses to wind power producers. Battery energy storage systems (BESS) that serve as onsite backup sources are among the solutions to mitigate wind curtailment. However, such an auxiliary role of the BESS might severely weaken its economic viability. This paper addresses the issue by proposing joint wind curtailment reduction and energy arbitrage for the BESS. We decouple the market participation of the co-located wind-battery system and develop a joint-bidding framework for the wind farm and BESS. It is challenging to optimize the joint-bidding because of the stochasticity of energy prices and wind generation. Therefore, we leverage deep reinforcement learning to maximize the overall revenue from the spot market while unlocking the BESS’s potential in concurrently reducing wind curtailment and conducting energy arbitrage. We validate the proposed strategy using realistic wind farm data and demonstrate that our joint-bidding strategy responds better to wind curtailment and generates higher revenues than the optimization-based benchmark. Our simulations also reveal that the extra wind generation used to be curtailed can be an effective power source to charge the BESS, resulting in additional financial returns.},
	booktitle = {2023 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting} ({PESGM})},
	author = {Li, Jinhao and Wang, Changlong and Wang, Hao},
	month = jul,
	year = {2023},
	note = {ISSN: 1944-9933},
	keywords = {Reinforcement learning, Deep reinforcement learning, Renewable energy sources, energy arbitrage, Deep learning, Economics, Climate change, Simulation, spot market, wind curtailment., Wind energy, Wind energy generation, wind-battery system},
	pages = {1--5},
	file = {Submitted Version:C\:\\Users\\Jacob\\Zotero\\storage\\7IQQQNI7\\Li et al. - 2023 - Optimal Energy Storage Scheduling for Wind Curtail.pdf:application/pdf},
}

@inproceedings{soofi_training_2023,
	title = {Training {A} {Deep} {Reinforcement} {Learning} {Agent} for {Microgrid} {Control} using {PSCAD} {Environment}},
	doi = {10.1109/GridEdge54130.2023.10102740},
	abstract = {The accessibility of real-time operational data along with breakthroughs in processing power have promoted the use of Machine Learning (ML) applications in current power systems. Prediction of device failures, meteorological data, system outages, and demand are among the applications of ML in the electricity grid. In this paper, a Reinforcement Learning (RL) method is utilized to design an efficient energy management system for grid-tied Energy Storage Systems (ESS). We implement a Deep Q-Learning (DQL) approach using Artificial Neural Networks (ANN) to design a microgrid controller system simulated in the PSCAD environment. The proposed on-grid controller coordinates the main grid, aggregated loads, renewable generations, and Advanced Energy Storage (AES). To reduce the cost of operating AESs, the designed controller takes the hourly energy market price into account in addition to physical system characteristics.},
	booktitle = {2023 {IEEE} {PES} {Grid} {Edge} {Technologies} {Conference} \& {Exposition} ({Grid} {Edge})},
	author = {Soofi, Arash Farokhi and Bayani, Reza and Yazdanibiouki, Mehrdad and Manshadi, Saeed D.},
	month = apr,
	year = {2023},
	keywords = {Q-learning, Real-time systems, Reinforcement learning, Training, Renewable energy sources, Microgrids, Artificial neural network, Artificial neural networks, Deep Q-learning, Distributed energy resources, Distributed power generation, Microgrid energy management system},
	pages = {1--5},
}

@inproceedings{thattai_consumer-centric_2023,
	title = {Consumer-{Centric} {Home} {Energy} {Management} {System} {Using} {Trust} {Region} {Policy} {Optimization}- {Based} {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	doi = {10.1109/PowerTech55446.2023.10202803},
	abstract = {Autonomous home energy management system (HEMS) is the key to improving energy efficiency in the active distribution network. This HEMS also needs to maintain customer satisfaction while maximizing cost savings under dynamic price conditions, incorporating uncertainties of consumer behavior, and renewable energy generation. In this paper, a consumer-centric HEMS using Trust Region Policy Optimization (TRPO) based multi-agent deep reinforcement learning (DRL) is presented. This Multi-Agent TRPO (MA- TRPO) based HEMS is trained to respond to the dynamic retail price and the local energy generation by scheduling the Interruptible-Deferrable load (IDA) and Battery Energy Storage System (BESS). Five-minute retail electricity price derived from wholesale market price and the PV generation data derived from real-world PV profiles are used to train the proposed MA- TRPO-based HEMS in discrete action space. The performance of the proposed HEMS is relatively better than the existing policy-gradient-based on-policy approaches such as Proximal Policy Optimization and Policy Gradient-based HEMS as validated via training and testing using the same dataset.},
	booktitle = {2023 {IEEE} {Belgrade} {PowerTech}},
	author = {Thattai, Kuthsav and Ravishankar, Jayashri and Li, Chaojie},
	month = jun,
	year = {2023},
	keywords = {Reinforcement learning, Deep reinforcement learning, Uncertainty, Training, Costs, Deep learning, Dynamic scheduling, Energy Management System, Multi-Agent, Smart Home, Trust region policy optimization, Washing machines},
	pages = {1--6},
	file = {Thattai et al. - 2023 - Consumer-Centric Home Energy Management System Usi.pdf:C\:\\Users\\Jacob\\Zotero\\storage\\PWRDCNYM\\Thattai et al. - 2023 - Consumer-Centric Home Energy Management System Usi.pdf:application/pdf},
}

@inproceedings{ochoa_efficient_2022,
	title = {Efficient {Bidding} of a {PV} {Power} {Plant} with {Energy} {Storage} {Participating} in {Day}-{Ahead} and {Real}-{Time} {Markets} {Using} {Artificial} {Neural} {Networks}},
	doi = {10.1109/PESGM48719.2022.9916732},
	abstract = {This paper proposes the use of Artificial Neural Networks (ANN) for the efficient bidding of a Photovoltaic power plant with Energy Storage System (PV-ESS) participating in Day-Ahead (DA) and Real-Time (RT) energy and reserve markets under uncertainty. The Energy Management System (EMS) is based on Multi-Agent Deep Reinforcement Learning (MADRL). The MADRL scheme aims to maximize the profit of the hybrid PV-ESS plant through an efficient bidding in both markets. Results show that the MADRL framework can fulfill both the financial and physical constraints faced by the PV-ESS plant while providing energy and ancillary services. Daily market incomes have comparable mean values regarding traditional optimization approaches (average value of 1839 USD), but with a 45.3\% smaller variance. Furthermore, it maintains a reference-tracking performance of 86.63\% for one-year-round participation, against a 73.05\% and 79.13\% performance obtained with scenario-based robust and stochastic programming implementations, respectively.},
	booktitle = {2022 {IEEE} {Power} \& {Energy} {Society} {General} {Meeting} ({PESGM})},
	author = {Ochoa, Tomás and Gil, Esteban and Angulo, Alejandro},
	month = jul,
	year = {2022},
	note = {ISSN: 1944-9933},
	keywords = {multi-agent deep reinforcement learning, Real-time systems, Reinforcement learning, Uncertainty, Stochastic processes, Artificial neural networks, energy storage, electricity markets, Photovoltaic systems, energy management system, Programming, solar generation},
	pages = {1--5},
	file = {Ochoa et al. - 2022 - Efficient Bidding of a PV Power Plant with Energy .pdf:C\:\\Users\\Jacob\\Zotero\\storage\\FSRQ2PV4\\Ochoa et al. - 2022 - Efficient Bidding of a PV Power Plant with Energy .pdf:application/pdf},
}
